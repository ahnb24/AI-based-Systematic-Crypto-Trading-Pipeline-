{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Trying to unpickle estimator StandardScaler from version\")\n",
    "from configs.history_data_crawlers_config import root_path\n",
    "from datetime import timedelta\n",
    "from realtime.realtime_dataset_functions import dataset_gen_realtime_loop, Error502_handling\n",
    "import numpy as np\n",
    "run_id = str(uuid.uuid4())\n",
    "\n",
    "from realtime.realtime_utils import (\n",
    "    add_RSI_to_realtime_dataset,\n",
    "    add_real_time_candles,\n",
    "    add_candle_fe,\n",
    "    add_fe_cndl_shift_fe_realtime_run,\n",
    "    add_fe_win_realtime_run,\n",
    "    add_fe_time_realtime_run,\n",
    "    add_fe_market_close_realtime_run,\n",
    "    get_coinex_time_now,\n",
    "    sleep_until_next_run,\n",
    "    predict_realtime,\n",
    "    add_ohlcv_non_w_to_realtime_dataset,\n",
    "    add_ohlcv_w_to_realtime_dataset,\n",
    "    add_78_to_realtime_dataset,\n",
    "    add_trade_features_to_realtime_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = get_coinex_time_now()\n",
    "type(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Initialize Reporter (Telegram)\n",
    "\n",
    "Configure your telegram bot variables. You need to set `TELEGRAM_CHAT_ID`, `TELEGRAM_CHAT_ID` and `TELEGRAM_BOT_TOKEN_SIGNAL` in the .env file. See [this tutorial](https://gist.github.com/nafiesl/4ad622f344cd1dc3bb1ecbe468ff9f8a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.telegram_functions import log_agent\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "\n",
    "\n",
    "bot_token = os.getenv('TELEGRAM_BOT_TOKEN')\n",
    "bot_token_signal = os.getenv('TELEGRAM_BOT_TOKEN_SIGNAL')\n",
    "chat_id = os.getenv('TELEGRAM_CHAT_ID')\n",
    "bot_token_signal_zaker = os.getenv('TELEGRAM_BOT_TOKEN_SIGNAL_ZAKER')\n",
    "chat_id_zaker = os.getenv('TELEGRAM_CHAT_ID_ZAKER')\n",
    "now = datetime.now(pytz.timezone('Asia/Tehran'))\n",
    "date_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "bot = log_agent(bot_token=bot_token,chat_id=chat_id, PRINT=True, TELEGRAM=True)\n",
    "bot_signal = log_agent(bot_token=bot_token_signal,chat_id=chat_id, PRINT=True, TELEGRAM=True)\n",
    "bot.send_message(text=\"ðŸš©\"*15)\n",
    "bot.send_message(text=\"ðŸ‘‡\"*15)\n",
    "bot.send_message(text=f\"start date_time ={date_string} - tehran time\")  \n",
    "bot_signal.send_message(text=f\"start date_time ={date_string} - tehran time\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load Model & Set Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from realtime.realtime_utils import load_models\n",
    "\n",
    "symbol_list = ['AAVEUSDT',]\n",
    "\n",
    "till_date = '2025-09-30'\n",
    "max_open_positions = 20\n",
    "\n",
    "models_list = load_models(symbol_list, till_date, max_open_positions, bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_open_positions_all = 0\n",
    "for model in models_list:\n",
    "    max_open_positions_all += 1\n",
    "max_open_positions_all *= 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Set Feature Creation Config\n",
    "\n",
    "### General Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.feature_configs_general import generate_general_config\n",
    "feature_config = generate_general_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_config.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Initialize Binance & Coinex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from trade_execution.coinex_trade_execution_functions import (\n",
    "    CoinexRequestsClient,\n",
    "    cal_candle_time_now,\n",
    "    get_spot_balance,\n",
    "    check_active_positions_for_time_force_close,\n",
    "    close_or_extend_expired_trades,\n",
    "    open_position,\n",
    "    check_tp_sl_positions,\n",
    "    close_and_exit,\n",
    "    cancel_all_orders,\n",
    "    get_open_stop_orders,\n",
    "    get_open_orders,\n",
    "    send_stop_order,\n",
    "    cancel_stop_order,\n",
    "    count_active_positions\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "bot.send_message(text=\"--> coinex connected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create dataset for the first time.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_size_in_days_ohlcv = 60\n",
    "data_size_in_days_trade = 3.5\n",
    "data_size_in_minutes_trade = data_size_in_days_trade*1440\n",
    "\n",
    "\n",
    "fe_functions = [\n",
    "    add_real_time_candles,\n",
    "    add_RSI_to_realtime_dataset,\n",
    "    add_fe_cndl_shift_fe_realtime_run,\n",
    "    add_candle_fe,\n",
    "    add_fe_time_realtime_run,\n",
    "    add_fe_win_realtime_run,\n",
    "    add_fe_market_close_realtime_run,\n",
    "    add_ohlcv_non_w_to_realtime_dataset,\n",
    "    add_ohlcv_w_to_realtime_dataset,\n",
    "    add_78_to_realtime_dataset,\n",
    "    add_trade_features_to_realtime_dataset\n",
    "]\n",
    "after_merge_functions = [\n",
    "  \n",
    "]\n",
    "\n",
    "\n",
    "dataset_config = {\n",
    "    \"features\": [\n",
    "        \"fe_cndl\",\n",
    "        \"fe_RSI\",\n",
    "        \"fe_ratio\",\n",
    "        \"fe_EMA\",\n",
    "        \"fe_SMA\",\n",
    "        \"fe_cndl_shift\",\n",
    "        \"fe_WIN\",\n",
    "        \"fe_ATR\",\n",
    "        \"fe_RSTD\",\n",
    "        \"fe_time\",\n",
    "        \"fe_market_close\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "datasetdict_path = f\"{root_path}/data/datasetdict.pkl\"\n",
    "if not os.path.exists(datasetdict_path):\n",
    "    dataset, final_df, crawl_time, shape, len_candle = dataset_gen_realtime_loop(\n",
    "    'init', fe_functions,feature_config, dataset_config, dataset={}, data_size_in_days_ohlcv=data_size_in_days_ohlcv,\\\n",
    "          data_size_in_days_trade=data_size_in_days_trade,\\\n",
    "          data_size_in_minutes_trade=data_size_in_minutes_trade)\n",
    "\n",
    "    dataset_dict = {\"dataset\": dataset,\n",
    "                    \"final_df\": final_df,\n",
    "                    \"crawl_time\": crawl_time,\n",
    "                    \"shape\": shape,\n",
    "                    \"len_candle\": len_candle}\n",
    "    with open(f'{root_path}/data/datasetdict.pkl', \"wb\") as f:\n",
    "        pickle.dump(dataset_dict, f)\n",
    "    del dataset_dict\n",
    "    bot.send_message(f\"--> final_df initialized till {final_df.index[-1].tz_localize('UTC')}.\")\n",
    "\n",
    "else:\n",
    "    with open(datasetdict_path, \"rb\") as f:\n",
    "        dataset_dict = pickle.load(f)\n",
    "        dataset = dataset_dict[\"dataset\"]\n",
    "        final_df = dataset_dict[\"final_df\"]\n",
    "        crawl_time = dataset_dict[\"crawl_time\"]\n",
    "        shape = dataset_dict[\"shape\"]\n",
    "        len_candle = dataset_dict[\"len_candle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (get_coinex_time_now() - final_df.index[-1].tz_localize('UTC')) >\\\n",
    "      timedelta(minutes=9.5):\n",
    "\n",
    "    print(\"indate\")\n",
    "    diff_min = (get_coinex_time_now() - final_df.index[-1].tz_localize('UTC'))\n",
    "    total_seconds = diff_min.total_seconds()\n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60) \n",
    "    indate_mins = int(np.ceil((hours*60 + minutes)/5) * 5)\n",
    "    dataset, final_df, crawl_time, shape, len_candle = dataset_gen_realtime_loop(\n",
    "        'indate',\n",
    "        fe_functions,\n",
    "        feature_config,\n",
    "        dataset_config,\n",
    "        dataset,\n",
    "        data_size_in_days_ohlcv=data_size_in_days_ohlcv,\n",
    "        data_size_in_days_trade=data_size_in_days_trade,\n",
    "        data_size_in_minutes_trade=data_size_in_minutes_trade,\n",
    "        shape=shape,\n",
    "        len_candle=len_candle,\n",
    "        indate_mins = indate_mins\n",
    "    )\n",
    "    dataset_dict = {\"dataset\": dataset,\n",
    "                    \"final_df\": final_df,\n",
    "                    \"crawl_time\": crawl_time,\n",
    "                    \"shape\": shape,\n",
    "                    \"len_candle\": len_candle}\n",
    "    with open(f'{root_path}/data/datasetdict.pkl', \"wb\") as f:\n",
    "        pickle.dump(dataset_dict, f)\n",
    "    del dataset_dict\n",
    "    bot.send_message(f\"--> final_df initialized till {final_df.index[-1].tz_localize('UTC')}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Realtime Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "prediction_data_path = f'{root_path}/data/realtime_trade_prediction_data.parquet'  # Replace with your actual file path\n",
    "\n",
    "try:\n",
    "  # Attempt to read the DataFrame from the file\n",
    "  if os.path.exists(prediction_data_path):\n",
    "    print(\"--> read realtime_trade_prediction_data:\")\n",
    "    preds_df = pd.read_parquet(prediction_data_path)\n",
    "  else:\n",
    "    print(\"--> no realtime_trade_prediction_data file. create an empty one:\")\n",
    "    preds_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred while reading the file: {e}\")\n",
    "  preds_df = pd.DataFrame()\n",
    "\n",
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "main loop predict\n",
    "\n",
    "\"\"\"\n",
    "import time as tt\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import subprocess\n",
    "\n",
    "\n",
    "run_every = 5 # run every X rounded minute\n",
    "offset_seconds = 1\n",
    "\n",
    "bot.send_message(f\"--> start the realtime loop, run every {run_every} minuts.\")\n",
    "while True:\n",
    "    try:\n",
    "        with open(f\"{root_path}/data/accounts.pkl\", \"rb\") as f:\n",
    "            accounts = pickle.load(f)\n",
    "        # Load all_trades if the file exists\n",
    "        all_trades_path = f\"{root_path}/data/all_trades.pkl\"\n",
    "        if os.path.exists(all_trades_path):\n",
    "            with open(all_trades_path, \"rb\") as f:\n",
    "                all_trades = pickle.load(f)\n",
    "        else:\n",
    "            all_trades = {} \n",
    "            for account in accounts.keys():\n",
    "                all_trades[account] = {}\n",
    "        \n",
    "        for account in accounts.keys():\n",
    "            if len(all_trades[account]) > 1000:\n",
    "                all_trades[account] = dict(list(all_trades[account].items())[-1000:])\n",
    "\n",
    "        \n",
    "        sleep_until_next_run(run_every = run_every , offset_seconds = offset_seconds,reporter=bot)\n",
    "        # bot.send_message(f\"============\")\n",
    "        bot.send_message(f\"ðŸ“ˆ \"*5)\n",
    "\n",
    "        #?? get data\n",
    "        dataset, final_df, crawl_time, shape, len_candle = dataset_gen_realtime_loop(\n",
    "        'update',\n",
    "        fe_functions,\n",
    "        feature_config,\n",
    "        dataset_config,\n",
    "        dataset,\n",
    "        data_size_in_days_ohlcv=data_size_in_days_ohlcv,\n",
    "        data_size_in_days_trade=data_size_in_days_trade,\n",
    "        data_size_in_minutes_trade=data_size_in_minutes_trade,\n",
    "        shape=shape,\n",
    "        len_candle=len_candle,\n",
    "        )\n",
    "        dataset_dict = {\"dataset\": dataset,\n",
    "                    \"final_df\": final_df,\n",
    "                    \"crawl_time\": crawl_time,\n",
    "                    \"shape\": shape,\n",
    "                    \"len_candle\": len_candle}\n",
    "        with open(f'{root_path}/data/datasetdict.pkl', \"wb\") as f:\n",
    "            pickle.dump(dataset_dict, f)\n",
    "        bot.send_message(f\"--> data updated.\")\n",
    "        del dataset_dict\n",
    "\n",
    "        expired_trades = {}\n",
    "        \n",
    "        #?? check for timing\n",
    "        for account in accounts.keys():\n",
    "            access_id = accounts[account][\"access_id\"]\n",
    "            secret_key = accounts[account][\"secret_key\"]\n",
    "            coinex_request = CoinexRequestsClient(access_id, secret_key)\n",
    "            expired_trades_list = []\n",
    "            \n",
    "            for coin in symbol_list:\n",
    "                # for model in coin:\n",
    "                expired_trades_sublist = []\n",
    "                symbol = coin\n",
    "                all_trades[account] = check_tp_sl_positions(coinex_request, all_trades[account], symbol)\n",
    "                # bot.send_message(f\"--> func check_tp_sl_positions {symbol} done.\")\n",
    "                all_trades[account], expired_trades_sublist = check_active_positions_for_time_force_close(account, coinex_request, all_trades[account], symbol, bot_signal)\n",
    "                # bot.send_message(f\"--> func check_active_positions_for_time_force_close {symbol} done.\")\n",
    "                expired_trades_list += expired_trades_sublist\n",
    "            \n",
    "            expired_trades[account] = expired_trades_list\n",
    "        #?? predict\n",
    "        predictions, preds_df = predict_realtime(models_list,predictions,preds_df,crawl_time,final_df,reporter=bot)\n",
    "\n",
    "        #?? trade\n",
    "        candle_now = cal_candle_time_now()\n",
    "        signals_df = preds_df.loc[(preds_df[\"_time\"] == candle_now ) & (preds_df[\"model_prediction\"] == 1)].drop_duplicates(\"model_id_name\",keep=\"last\")\n",
    "        \n",
    "        for account in accounts.keys():\n",
    "            access_id = accounts[account][\"access_id\"]\n",
    "            secret_key = accounts[account][\"secret_key\"]\n",
    "            coinex_request = CoinexRequestsClient(access_id, secret_key)\n",
    "            all_trades[account], signal_dff = close_or_extend_expired_trades(account, coinex_request, signals_df, expired_trades[account], all_trades[account], bot_signal)\n",
    "            # bot.send_message(f\"--> func close_or_extend_expired_trades {account} done.\")\n",
    "            if signal_dff.shape[0]>0:\n",
    "                # bot.send_message(f\"--> shape signals_df: {signals_df.shape[0]}.\")\n",
    "                pred_time = signal_dff.iloc[0]['predict_time'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                # bot.send_message(f\"--> predict_time: {pred_time}.\")\n",
    "                bot_signal.send_message(f\"--> predict_time: {pred_time}.\")\n",
    "                # bot_signal_zaker.send_message(f\"--> predict_time: {pred_time}.\")\n",
    "            for _,row in signal_dff.iterrows():\n",
    "                trade_side = row[\"strategy_trade_mode\"]\n",
    "                symbol = row[\"strategy_target_symbol\"].upper()\n",
    "                tp_percent = row[\"strategy_take_profit\"]\n",
    "                sl_percent = row[\"strategy_stop_loss\"]\n",
    "                look_ahead = row[\"strategy_look_ahead\"] ##?? in minutes\n",
    "                max_open_positions = row[\"strategy_max_open_positions\"]\n",
    "                last_candle_close_price = dataset[\"st_one\"][symbol].loc[dataset[\"st_one\"][symbol][\"_time\"]==cal_candle_time_now()][\"close\"].values[0]\n",
    "                with open(f'{root_path}/data/trade_permission.pkl', 'rb') as f:\n",
    "                    trade_permission = pickle.load(f)\n",
    "                if trade_permission[\"P\"] == 1:\n",
    "                    all_trades[account] = open_position(\n",
    "                        account,\n",
    "                        coinex_request,\n",
    "                        bot_signal,\n",
    "                        trade_side,\n",
    "                        symbol,\n",
    "                        tp_percent,\n",
    "                        sl_percent,\n",
    "                        last_candle_close_price,\n",
    "                        base_price_mode=\"tick_price\",\n",
    "                        all_trades=all_trades[account],\n",
    "                        max_open_positions=max_open_positions,\n",
    "                        max_open_positions_all=max_open_positions_all,\n",
    "                        look_ahead_minutes=look_ahead)\n",
    "            \n",
    "            now = get_coinex_time_now()\n",
    "            if (now.hour == 21 and now.minute > 35 and now.minute < 40) or \\\n",
    "                (now.hour == 9 and now.minute > 35 and now.minute < 40):\n",
    "                active = {}  # Active positions\n",
    "                not_active = {}  # Inactive positions\n",
    "\n",
    "                for order_id, order_info in all_trades[account].items():\n",
    "                    if order_info.get('position_status_active'):\n",
    "                        active[order_id] = order_info\n",
    "                    else:\n",
    "                        not_active[order_id] = order_info\n",
    "\n",
    "                # now = get_coinex_time_now()\n",
    "                now = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                now = now.replace(\":\", \"_\")\n",
    "                # Make sure the directory exists\n",
    "                directory = f\"{root_path}/data/saved_all_trades/{account}\"\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "                # Save the pickle file\n",
    "                with open(f\"{directory}/all_trades_{account}_till_{now}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(not_active, f)\n",
    "                all_trades[account] = active\n",
    "                del active\n",
    "                # saved_all_trade[account] == True\n",
    "                bot.send_message(f\"--> all_trades saved and flushed for {account}.\")\n",
    "\n",
    "            \n",
    "        bot.send_message(f\"--> loop end.\")\n",
    "\n",
    "        preds_df.to_parquet(f\"{root_path}/data/preds_df.parquet\")\n",
    "        with open(all_trades_path, \"wb\") as f:\n",
    "            pickle.dump(all_trades, f)\n",
    "        del all_trades\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        err_str = str(e)\n",
    "        if \"502 Bad Gateway\" in err_str or \"The request could not be satisfied\" in err_str:\n",
    "        # if True:\n",
    "            bot.send_message(\"!!! 502 Bad Gateway detected. Modifying dataset and restarting...\")\n",
    "\n",
    "            Error502_handling()\n",
    "            tt.sleep(20)\n",
    "            \n",
    "            sys.exit(1)  # exit current process so the new one takes over\n",
    "        \n",
    "        elif 'invalid slice' in err_str:\n",
    "            bot.send_message(\"!!! invalid slice argument detected. Modifying dataset and restarting...\")\n",
    "            \n",
    "            Error502_handling()\n",
    "            tt.sleep(20)\n",
    "            \n",
    "            sys.exit(1)  # exit current process so the new one takes over\n",
    "\n",
    "        elif 'Wrong timing' in err_str:\n",
    "            bot.send_message(\"!!! Wrong timing. Modifying dataset and restarting...\")\n",
    "            \n",
    "            Error502_handling()\n",
    "            tt.sleep(20)\n",
    "            \n",
    "            sys.exit(1)  # exit current process so the new one takes over\n",
    "\n",
    "        else:\n",
    "            bot.send_message(f\"!!! ERROR: {err_str}\")\n",
    "            traceback.print_exc()\n",
    "            bot.send_message(traceback.format_exc())\n",
    "            tt.sleep(60)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
